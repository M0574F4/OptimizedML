{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Running command:\n",
      " \n",
      "python -m OptimizedML.quantize_ptq \\\n",
      "    --model_name resnet18 \\\n",
      "    --data_path ../data \\\n",
      "    --output_dir ../quantized_models \\\n",
      "    --backend fbgemm \\\n",
      "    --num_calib_samples 100 \\\n",
      "    --batch_size 32 \\\n",
      "    --evaluate\n",
      "\n",
      "--- Starting PTQ Static Quantization for resnet18 ---\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "Original FP32 resnet18 model loaded and moved to CPU.\n",
      "Preprocessing transforms for model loaded.\n",
      "Quantization-aware resnet18 architecture created and weights loaded.\n",
      "\n",
      "--- Configuring Quantization ---\n",
      "Quantization backend set to: fbgemm\n",
      "Quantization configuration applied to the model.\n",
      "Attempting module fusion...\n",
      "torch.quantization.fuse_modules_qat not found, trying fuse_modules...\n",
      "Attempted fusion with basic fuse_modules (may require specific layer lists).\n",
      "--- Preparing Calibration Data (using CIFAR10 from ../data) ---\n",
      "100.0%\n",
      "Using 100 images from CIFAR10 for calibration.\n",
      "Calibration DataLoader created with batch size 32.\n",
      "Sample batch tensor shape: torch.Size([32, 3, 224, 224]), dtype: torch.float32\n",
      "\n",
      "--- Preparing Model for Static Quantization (PTQ) ---\n",
      "/project_ghent/Mostafa/OptimizedML/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "Model prepared for static quantization (observers inserted).\n",
      "\n",
      "--- Calibrating the Model ---\n",
      "Running calibration data through the prepared model...\n",
      "  Calibration batch 4/4 processed.\n",
      "Calibration finished. Activation statistics collected.\n",
      "\n",
      "--- Converting the Model to Quantized INT8 ---\n",
      "Model successfully converted to INT8 quantized format.\n",
      "Quantized INT8 model state_dict saved to: ../quantized_models/resnet18_int8_quantized.pt\n",
      "\n",
      "--- Comparing Model Performance ---\n",
      "\n",
      "Comparing Model Sizes:\n",
      "FP32_Original Model size: 44.67 MB\n",
      "INT8_Quantized Model size: 11.39 MB\n",
      "\n",
      "Size reduction factor: 3.92x\n",
      "Model size reduced from 44.67 MB to 11.39 MB.\n",
      "\n",
      "Comparing Inference Speed (CPU):\n",
      "Using sample input batch of shape: torch.Size([32, 3, 224, 224]) on CPU for timing.\n",
      "\n",
      "Timing Original FP32 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "  Avg time: 5095.486 ms, Std Dev: 252.680 ms\n",
      "Average FP32 inference time: 5095.486 ms per batch\n",
      "\n",
      "Timing INT8 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "  Avg time: 2561.200 ms, Std Dev: 232.506 ms\n",
      "Average INT8 inference time: 2561.200 ms per batch\n",
      "\n",
      "Inference speedup factor (INT8 vs FP32 on CPU): 1.99x\n",
      "\n",
      "--- Quantization process for resnet18 finished. ---\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'OptimizedML'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 112\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchvision\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ResNet18_Weights\n\u001b[32m    111\u001b[39m \u001b[38;5;66;03m# CORRECT\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mOptimizedML\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_quantization_aware_model\n\u001b[32m    113\u001b[39m \u001b[38;5;66;03m# Define path to the saved state dict\u001b[39;00m\n\u001b[32m    114\u001b[39m quantized_model_path = os.path.join(output_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_int8_quantized.pt\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'OptimizedML'"
     ]
    }
   ],
   "source": [
    "# # Demonstration: Using the PTQ Static Quantization Script\n",
    "#\n",
    "# This notebook demonstrates how to use the `quantize_ptq.py` script to perform\n",
    "# Post-Training Static Quantization on a supported model (e.g., ResNet18).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Define paths relative to the notebook location (assuming it's in the repo root)\n",
    "output_dir = \"quantized_models\"\n",
    "data_dir = \"data\"\n",
    "\n",
    "# Create output and data directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# ## 2. Running the Quantization Script\n",
    "#\n",
    "# We will use the command line interface of `quantize_ptq.py`.\n",
    "#\n",
    "# **Arguments:**\n",
    "# * `--model_name`: The model to quantize (e.g., `resnet18`).\n",
    "# * `--data_path`: Path where calibration data (CIFAR10) will be stored/loaded.\n",
    "# * `--output_dir`: Path where the quantized model state_dict will be saved.\n",
    "# * `--backend`: Quantization backend (`fbgemm` for x86 CPU is recommended).\n",
    "# * `--num_calib_samples`: Number of images for calibration (e.g., 100 for a quick demo, 500+ recommended).\n",
    "# * `--batch_size`: Batch size for calibration.\n",
    "# * `--evaluate`: Flag to run size and speed comparison after quantization.\n",
    "\n",
    "# %%\n",
    "# Define parameters for the script\n",
    "model_name = \"resnet18\"\n",
    "num_calibration_samples = 100 # Use a smaller number for a quick demo run\n",
    "batch_size = 32\n",
    "backend = \"fbgemm\" # Use 'qnnpack' if on ARM\n",
    "\n",
    "\n",
    "# --- Define paths RELATIVE to the notebook in /notebooks/ ---\n",
    "# Go one level up to the repo root to find 'data' and 'quantized_models'\n",
    "repo_root = \"..\"\n",
    "output_dir = os.path.join(repo_root, \"quantized_models\")\n",
    "data_dir = os.path.join(repo_root, \"data\")\n",
    "\n",
    "# Create directories if they don't exist relative to repo root\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# NEW Command using python -m\n",
    "# Assumes quantize_ptq.py is directly inside the OptimizedML package directory\n",
    "module_path = \"OptimizedML.quantize_ptq\"\n",
    "\n",
    "command = f\"\"\"\n",
    "python -m {module_path} \\\\\n",
    "    --model_name {model_name} \\\\\n",
    "    --data_path {data_dir} \\\\\n",
    "    --output_dir {output_dir} \\\\\n",
    "    --backend {backend} \\\\\n",
    "    --num_calib_samples {num_calibration_samples} \\\\\n",
    "    --batch_size {batch_size} \\\\\n",
    "    --evaluate\n",
    "\"\"\"\n",
    "\n",
    "print(\"Running command:\\n\", command)\n",
    "\n",
    "# Execute the command (no change here)\n",
    "# !{command}\n",
    "\n",
    "# Execute the command\n",
    "# Note: The output will appear in the notebook's output cell.\n",
    "!{command}\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Loading and Using the Quantized Model\n",
    "#\n",
    "# After the script finishes successfully, the quantized model's `state_dict` is saved (e.g., in `quantized_models/resnet18_int8_quantized.pt`).\n",
    "#\n",
    "# To use it, you need to:\n",
    "# 1. Create an instance of the quantization-aware model architecture (`models_quant.resnet18`).\n",
    "# 2. Set its `qconfig` (although not strictly necessary for inference if already converted).\n",
    "# 3. **Crucially, prepare and convert this architecture instance *without calibration***. This sets up the correct quantized layers (like `QuantizedConv2d`). Alternatively, ensure the architecture is set to `quantize=True` if the model supports it.\n",
    "# 4. Load the saved `state_dict`.\n",
    "\n",
    "# %%\n",
    "import torchvision.models.quantization as models_quant\n",
    "from torchvision.models import ResNet18_Weights\n",
    "# CORRECT\n",
    "from OptimizedML.model_utils import get_quantization_aware_model\n",
    "# Define path to the saved state dict\n",
    "quantized_model_path = os.path.join(output_dir, f\"{model_name}_int8_quantized.pt\")\n",
    "\n",
    "if os.path.exists(quantized_model_path):\n",
    "    print(f\"Loading quantized state_dict from: {quantized_model_path}\")\n",
    "\n",
    "    # 1. Create the quantization-aware architecture instance\n",
    "    # We don't need the original FP32 model here, just the architecture\n",
    "    # IMPORTANT: Need to ensure the architecture is ready to accept quantized weights.\n",
    "    # For models_quant, often creating it and then applying convert is simplest.\n",
    "    model_arch = models_quant.resnet18(weights=None, quantize=False) # Start with FP32 quant-aware arch\n",
    "    model_arch.eval()\n",
    "    model_arch.cpu()\n",
    "\n",
    "    # 2. Apply necessary quantization setup (qconfig, prepare, convert)\n",
    "    # This step is crucial to transform the nn.Conv2d layers etc. into QuantizedConv2d etc.\n",
    "    # before loading the INT8 state dict.\n",
    "    qconfig = torch.quantization.get_default_qconfig(backend) # Use the same backend\n",
    "    model_arch.qconfig = qconfig\n",
    "    # No need to fuse again if loading state_dict for a converted model.\n",
    "    torch.quantization.prepare(model_arch, inplace=True) # Prepare (inserts observers)\n",
    "    # *** Convert the architecture WITHOUT calibration ***\n",
    "    torch.quantization.convert(model_arch, inplace=True)\n",
    "    print(\"Prepared and converted model architecture to INT8 format.\")\n",
    "\n",
    "    # 3. Load the saved INT8 state dictionary\n",
    "    int8_state_dict = torch.load(quantized_model_path, map_location='cpu')\n",
    "    model_arch.load_state_dict(int8_state_dict)\n",
    "    int8_loaded_model = model_arch # Assign for clarity\n",
    "    int8_loaded_model.eval() # Ensure eval mode\n",
    "\n",
    "    print(\"Successfully loaded INT8 state_dict into the converted architecture.\")\n",
    "\n",
    "    # 4. Optional: Test Inference with a dummy input\n",
    "    # Get the preprocessing transform\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "    dummy_input_tensor = torch.randn(1, 3, 224, 224) # Batch size 1\n",
    "    # Apply preprocessing\n",
    "    # dummy_input_processed = preprocess(dummy_input_tensor) # Preprocessing expects PIL Image or Tensor[C, H, W] usually\n",
    "    # For a raw tensor, ensure normalization matches if needed, or just test the forward pass\n",
    "    # Note: preprocess often includes ToTensor which converts HWC uint8 to CHW float\n",
    "    # Since our dummy is already CHW float, we might skip parts of preprocess or adjust.\n",
    "    # For simplicity, just use the raw tensor shape. Real data needs the full preprocess.\n",
    "\n",
    "    print(\"\\nTesting inference with dummy input...\")\n",
    "    with torch.no_grad():\n",
    "        output = int8_loaded_model(dummy_input_tensor)\n",
    "    print(\"Inference successful!\")\n",
    "    print(\"Output shape:\", output.shape)\n",
    "\n",
    "else:\n",
    "    print(f\"Quantized model state_dict not found at: {quantized_model_path}\")\n",
    "    print(\"Please run the quantization script first.\")\n",
    "\n",
    "# %% [markdown]\n",
    "# This notebook provides a basic template for using the quantization script and loading the resulting model. You can adapt the script calls and loading steps as needed for your specific use case and models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
