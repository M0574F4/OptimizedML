{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Torchvision Version: 0.21.0+cu124\n",
      "Using device: cuda\n",
      "Original FP32 ResNet18 model loaded and moved to CPU.\n",
      "Preprocessing transforms for model loaded.\n",
      "\n",
      "--- Checking Initial Model Size ---\n",
      "FP32_Original Model size: 44.67 MB\n",
      "\n",
      "--- Preparing Quantization-Aware Model ---\n",
      "Quantization-aware ResNet18 architecture created.\n",
      "Loaded weights from the original FP32 model.\n",
      "\n",
      "--- Configuring Quantization ---\n",
      "Quantization backend set to: fbgemm\n",
      "Quantization configuration applied to the model.\n",
      "\n",
      "--- Fusing Modules ---\n",
      "Could not fuse modules (might be ok if model doesn't have standard patterns): module 'torch.quantization' has no attribute 'fuse_modules_qat'\n",
      "\n",
      "--- Preparing Calibration Data ---\n",
      "Using 500 images from CIFAR10 for calibration.\n",
      "Calibration DataLoader created with batch size 32.\n",
      "Sample batch tensor shape: torch.Size([32, 3, 224, 224]), dtype: torch.float32\n",
      "\n",
      "--- Preparing Model for Static Quantization ---\n",
      "Model prepared for static quantization (observers inserted).\n",
      "\n",
      "--- Calibrating the Model ---\n",
      "Running calibration data through the prepared model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/Mostafa/OptimizedML/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Calibration batch 16/16 processed.\n",
      "Calibration finished. Activation statistics collected by observers.\n",
      "\n",
      "--- Converting the Model to Quantized INT8 ---\n",
      "Model successfully converted to INT8 quantized format.\n",
      "\n",
      "--- Comparing Model Sizes ---\n",
      "Original FP32 Model:\n",
      "FP32_Original Model size: 44.67 MB\n",
      "\n",
      "Quantized INT8 Model:\n",
      "INT8 Model size: 11.38 MB\n",
      "\n",
      "Size reduction factor: 3.92x\n",
      "Model size reduced from 44.67 MB to 11.38 MB.\n",
      "\n",
      "--- Comparing Inference Speed (CPU) ---\n",
      "Using quantization backend: fbgemm\n",
      "Using sample input batch of shape: torch.Size([32, 3, 224, 224]) on CPU for timing.\n",
      "\n",
      "Timing Original FP32 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "  Avg time: 6754.067 ms, Std Dev: 9240.614 ms\n",
      "Average FP32 inference time: 6754.067 ms per batch\n",
      "\n",
      "Timing INT8 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "  Avg time: 2569.394 ms, Std Dev: 239.379 ms\n",
      "Average INT8 inference time: 2569.394 ms per batch\n",
      "\n",
      "Inference speedup factor (INT8 vs FP32 on CPU): 2.63x\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports and Setup\n",
    "# ==========================\n",
    "import torch\n",
    "import torchvision\n",
    "# Import the specific quantization models module\n",
    "import torchvision.models.quantization as models_quant\n",
    "from torchvision.models import ResNet18_Weights # Use the modern weights API\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "# Check for CUDA availability (optional but good practice)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # Quantization primarily targets CPU\n",
    "\n",
    "# Cell 2: Load Original FP32 Model (for weights and comparison)\n",
    "# ==============================================================\n",
    "def load_original_model():\n",
    "    \"\"\"Loads the pre-trained FP32 ResNet18 model.\"\"\"\n",
    "    weights = ResNet18_Weights.DEFAULT # Loads the best available weights (ImageNet V1)\n",
    "    model = torchvision.models.resnet18(weights=weights)\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.cpu() # Ensure it's on CPU\n",
    "    print(\"Original FP32 ResNet18 model loaded and moved to CPU.\")\n",
    "    # Get the transformation pipeline associated with the weights\n",
    "    preprocess = weights.transforms()\n",
    "    print(\"Preprocessing transforms for model loaded.\")\n",
    "    return model, preprocess\n",
    "\n",
    "# Load the original floating-point model\n",
    "fp32_model, preprocess = load_original_model()\n",
    "\n",
    "# Cell 3: Helper function for Model Size\n",
    "# =======================================\n",
    "def print_model_size(model, label=\"\"):\n",
    "    \"\"\"Saves the model's state_dict temporarily and prints its size.\"\"\"\n",
    "    temp_file_path = f\"{label}_temp_model_state.pth\" # Use label in filename\n",
    "    torch.save(model.state_dict(), temp_file_path)\n",
    "    size_bytes = os.path.getsize(temp_file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    print(f\"{label} Model size: {size_mb:.2f} MB\")\n",
    "    os.remove(temp_file_path)\n",
    "    return size_mb\n",
    "\n",
    "# Check the size of the original FP32 model\n",
    "print(\"\\n--- Checking Initial Model Size ---\")\n",
    "fp32_model_size = print_model_size(fp32_model, \"FP32_Original\")\n",
    "\n",
    "\n",
    "# Cell 4: Prepare Quantization-Aware Model Architecture\n",
    "# ======================================================\n",
    "print(\"\\n--- Preparing Quantization-Aware Model ---\")\n",
    "\n",
    "# Create an instance of the quantization-aware ResNet18 architecture.\n",
    "# `quantize=False` initially means it's FP32 but has the structure (stubs, etc.) for quantization.\n",
    "# We don't pass weights here; we'll load them from our original fp32_model.\n",
    "model_to_quantize = models_quant.resnet18(weights=None, quantize=False) # ***MODIFIED***\n",
    "\n",
    "# Load the state dictionary from the original FP32 model into the quantization-aware architecture.\n",
    "# This transfers the learned weights. `strict=True` is default and should work.\n",
    "model_to_quantize.load_state_dict(fp32_model.state_dict()) # ***MODIFIED***\n",
    "model_to_quantize.eval() # Set to evaluation mode\n",
    "model_to_quantize.cpu() # Ensure it's on CPU\n",
    "\n",
    "print(\"Quantization-aware ResNet18 architecture created.\")\n",
    "print(\"Loaded weights from the original FP32 model.\")\n",
    "\n",
    "\n",
    "# Cell 5: Configure Quantization Backend and QConfig\n",
    "# ==================================================\n",
    "print(\"\\n--- Configuring Quantization ---\")\n",
    "\n",
    "q_backend = \"none\"\n",
    "# Check for supported engines (prefer fbgemm for x86)\n",
    "if 'fbgemm' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"fbgemm\"\n",
    "elif 'qnnpack' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"qnnpack\"\n",
    "else:\n",
    "    print(\"Warning: Neither 'fbgemm' nor 'qnnpack' supported. Static quantization might not work well.\")\n",
    "\n",
    "qconfig = None\n",
    "if q_backend != \"none\":\n",
    "    try:\n",
    "        qconfig = torch.quantization.get_default_qconfig(q_backend)\n",
    "        torch.backends.quantized.engine = q_backend\n",
    "        print(f\"Quantization backend set to: {q_backend}\")\n",
    "\n",
    "        # Apply the qconfig to the quantization-aware model instance\n",
    "        # Note: Fusion might be applied later, but qconfig needs setting first.\n",
    "        model_to_quantize.qconfig = qconfig # ***MODIFIED*** (applied to model_to_quantize)\n",
    "        print(\"Quantization configuration applied to the model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up quantization backend {q_backend}: {e}\")\n",
    "        qconfig = None\n",
    "else:\n",
    "    print(\"Skipping quantization configuration due to lack of supported backend.\")\n",
    "\n",
    "\n",
    "# Cell 6: Fuse Modules (Important for Quantization Performance)\n",
    "# =============================================================\n",
    "# Even though models_quant.resnet18 defines potential fusions,\n",
    "# we typically still need to explicitly call fuse_modules after applying qconfig.\n",
    "# This modifies the model_to_quantize in place.\n",
    "print(\"\\n--- Fusing Modules ---\")\n",
    "if qconfig:\n",
    "    model_to_quantize.eval() # Fuse expects eval mode\n",
    "    # The quantization-aware models often have internal flags or methods,\n",
    "    # but explicit fusion is standard practice before prepare.\n",
    "    # Let's attempt fusing common patterns like Conv-BN-ReLU\n",
    "    try:\n",
    "        # fuse_modules typically modifies the model in-place\n",
    "        # We fuse the model_to_quantize *before* preparing it\n",
    "        torch.quantization.fuse_modules_qat(model_to_quantize, inplace=True) # Use QAT version for safety even in PTQ workflow with standard layers\n",
    "        # Or use fuse_modules if specifically targetting PTQ patterns and fuse_modules_qat causes issues\n",
    "        # torch.quantization.fuse_modules(model_to_quantize, [['conv1', 'bn1', 'relu']], inplace=True) # Example specific fusion\n",
    "        print(\"Attempted module fusion on the model.\")\n",
    "        # NOTE: The exact fusion list might need adjustment depending on the model\n",
    "        # and desired fusion patterns. models_quant.resnet18 is designed for common fusions.\n",
    "    except Exception as e:\n",
    "        print(f\"Could not fuse modules (might be ok if model doesn't have standard patterns): {e}\")\n",
    "else:\n",
    "    print(\"Skipping fusion because qconfig was not set.\")\n",
    "\n",
    "\n",
    "# Cell 7: Prepare Calibration Data (No Changes Needed Here)\n",
    "# =========================================================\n",
    "print(\"\\n--- Preparing Calibration Data ---\")\n",
    "\n",
    "# Use CIFAR10 dataset for calibration images\n",
    "calibration_transform = preprocess # Use transforms from original model\n",
    "\n",
    "data_dir = './data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Created directory: {data_dir}\")\n",
    "\n",
    "calibration_loader = None\n",
    "try:\n",
    "    calibration_dataset_full = datasets.CIFAR10(\n",
    "        root=data_dir, train=True, download=True, transform=calibration_transform\n",
    "    )\n",
    "    num_calibration_images = 500 # Use a subset\n",
    "    calibration_subset_indices = list(range(num_calibration_images))\n",
    "    calibration_dataset = torch.utils.data.Subset(calibration_dataset_full, calibration_subset_indices)\n",
    "    calibration_loader = torch.utils.data.DataLoader(\n",
    "        calibration_dataset, batch_size=32, shuffle=False, num_workers=2\n",
    "    )\n",
    "    print(f\"Using {len(calibration_dataset)} images from CIFAR10 for calibration.\")\n",
    "    print(f\"Calibration DataLoader created with batch size {calibration_loader.batch_size}.\")\n",
    "    images, _ = next(iter(calibration_loader))\n",
    "    print(f\"Sample batch tensor shape: {images.shape}, dtype: {images.dtype}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading or processing calibration data: {e}\")\n",
    "    calibration_loader = None\n",
    "\n",
    "\n",
    "# Cell 8: Prepare Model for Static Quantization (PTQ)\n",
    "# ===================================================\n",
    "print(\"\\n--- Preparing Model for Static Quantization ---\")\n",
    "\n",
    "prepared_model = None # Initialize\n",
    "prepared_model_ready = False\n",
    "if qconfig and calibration_loader and model_to_quantize: # Check model_to_quantize existence\n",
    "    model_to_quantize.cpu().eval() # Ensure CPU and eval mode\n",
    "    # Use torch.quantization.prepare for Post-Training Quantization\n",
    "    # This inserts observers based on the qconfig. Operates inplace.\n",
    "    torch.quantization.prepare(model_to_quantize, inplace=True) # ***MODIFIED*** (applied to model_to_quantize)\n",
    "    prepared_model = model_to_quantize # Assign for clarity\n",
    "    print(\"Model prepared for static quantization (observers inserted).\")\n",
    "    prepared_model_ready = True\n",
    "else:\n",
    "    print(\"Skipping model preparation: Check qconfig, calibration data, and model definition.\")\n",
    "\n",
    "\n",
    "# Cell 9: Calibrate the Model\n",
    "# ===========================\n",
    "print(\"\\n--- Calibrating the Model ---\")\n",
    "\n",
    "calibration_done = False\n",
    "if prepared_model_ready and prepared_model:\n",
    "    print(\"Running calibration data through the prepared model...\")\n",
    "    prepared_model.cpu().eval() # Ensure CPU and eval mode\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _) in enumerate(calibration_loader):\n",
    "            images_cpu = images.to('cpu')\n",
    "            prepared_model(images_cpu) # Feed data to the *prepared* model\n",
    "            print(f\"  Calibration batch {i+1}/{len(calibration_loader)} processed.\", end='\\r')\n",
    "\n",
    "    print(\"\\nCalibration finished. Activation statistics collected by observers.\")\n",
    "    calibration_done = True\n",
    "else:\n",
    "    print(\"Skipping calibration step because model was not prepared successfully.\")\n",
    "\n",
    "\n",
    "# Cell 10: Convert Model to Quantized INT8\n",
    "# =======================================\n",
    "print(\"\\n--- Converting the Model to Quantized INT8 ---\")\n",
    "\n",
    "int8_model = None # Initialize variable\n",
    "conversion_done = False\n",
    "\n",
    "if calibration_done and prepared_model:\n",
    "    prepared_model.cpu().eval() # Ensure CPU and eval mode before conversion\n",
    "    try:\n",
    "        # Convert the calibrated model. Operates inplace by default.\n",
    "        # Assign to int8_model for clarity, although it modifies prepared_model.\n",
    "        torch.quantization.convert(prepared_model, inplace=True) # ***MODIFIED*** (applied to prepared_model)\n",
    "        int8_model = prepared_model # prepared_model is now the converted int8 model\n",
    "        print(\"Model successfully converted to INT8 quantized format.\")\n",
    "        conversion_done = True\n",
    "        # Optional: Print the structure\n",
    "        # print(\"\\nStructure of the INT8 Model:\")\n",
    "        # print(int8_model)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model conversion: {e}\")\n",
    "        conversion_done = False\n",
    "else:\n",
    "    print(\"Skipping conversion because calibration was not completed successfully or prepared model missing.\")\n",
    "\n",
    "\n",
    "# Cell 11: Compare Model Sizes\n",
    "# ============================\n",
    "print(\"\\n--- Comparing Model Sizes ---\")\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    print(\"Original FP32 Model:\")\n",
    "    # Use the size calculated earlier or recalculate\n",
    "    fp32_model_size_check = print_model_size(fp32_model, \"FP32_Original\") # Use original fp32 model\n",
    "\n",
    "    print(\"\\nQuantized INT8 Model:\")\n",
    "    int8_model_size = print_model_size(int8_model, \"INT8\") # Use the converted int8_model\n",
    "\n",
    "    if int8_model_size > 0 and fp32_model_size_check > 0:\n",
    "      size_reduction = fp32_model_size_check / int8_model_size\n",
    "      print(f\"\\nSize reduction factor: {size_reduction:.2f}x\")\n",
    "      print(f\"Model size reduced from {fp32_model_size_check:.2f} MB to {int8_model_size:.2f} MB.\")\n",
    "    else:\n",
    "      print(\"\\nCould not calculate size reduction (one or both model sizes are zero or invalid).\")\n",
    "else:\n",
    "    print(\"Skipping size comparison because conversion step failed or was skipped.\")\n",
    "\n",
    "\n",
    "# Cell 12: Compare Inference Speed (CPU) - UPDATED\n",
    "# ================================================\n",
    "print(\"\\n--- Comparing Inference Speed (CPU) ---\")\n",
    "\n",
    "# --- Explicitly set quantization backend --- (Should match what was used for qconfig)\n",
    "try:\n",
    "    current_backend = torch.backends.quantized.engine\n",
    "    print(f\"Using quantization backend: {current_backend}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not verify quantization backend. Error: {e}\")\n",
    "    current_backend = q_backend # Fallback to the intended backend\n",
    "    print(f\"Assuming backend is: {current_backend}\")\n",
    "\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    # Ensure both models are on CPU and in eval mode\n",
    "    fp32_model.cpu().eval() # Original FP32 model\n",
    "    int8_model.cpu().eval() # Converted INT8 model\n",
    "\n",
    "    # Create a sample input tensor (using one batch from the calibration loader)\n",
    "    try:\n",
    "        # Re-create iterator in case it was exhausted\n",
    "        calib_iter = iter(calibration_loader)\n",
    "        sample_input, _ = next(calib_iter)\n",
    "        sample_input_cpu = sample_input.to('cpu')\n",
    "        print(f\"Using sample input batch of shape: {sample_input_cpu.shape} on CPU for timing.\")\n",
    "\n",
    "        # Helper function to time inference runs accurately\n",
    "        def time_model_inference(model, input_tensor, num_runs=50, warm_up=10):\n",
    "            model.eval()\n",
    "            model.to('cpu')\n",
    "            input_tensor = input_tensor.to('cpu')\n",
    "            times = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Warm-up runs\n",
    "                print(f\"  Performing {warm_up} warm-up runs...\")\n",
    "                for _ in range(warm_up):\n",
    "                    _ = model(input_tensor)\n",
    "\n",
    "                # Timed runs\n",
    "                print(f\"  Performing {num_runs} timed runs...\")\n",
    "                for _ in range(num_runs):\n",
    "                    start_time = time.time()\n",
    "                    _ = model(input_tensor)\n",
    "                    end_time = time.time()\n",
    "                    times.append((end_time - start_time) * 1000) # Store time in milliseconds\n",
    "\n",
    "            avg_time_ms = np.mean(times)\n",
    "            std_dev_ms = np.std(times)\n",
    "            print(f\"  Avg time: {avg_time_ms:.3f} ms, Std Dev: {std_dev_ms:.3f} ms\")\n",
    "            return avg_time_ms\n",
    "\n",
    "        # --- Time FP32 model inference ---\n",
    "        print(\"\\nTiming Original FP32 model inference...\")\n",
    "        fp32_avg_time = time_model_inference(fp32_model, sample_input_cpu)\n",
    "        print(f\"Average FP32 inference time: {fp32_avg_time:.3f} ms per batch\")\n",
    "\n",
    "        # --- Time INT8 model inference ---\n",
    "        print(\"\\nTiming INT8 model inference...\")\n",
    "        # Ensure PyTorch threading is set for optimal performance (often helps INT8)\n",
    "        # torch.set_num_threads(1) # Optional: test single-thread performance\n",
    "        int8_avg_time = time_model_inference(int8_model, sample_input_cpu)\n",
    "        # torch.set_num_threads(torch.get_num_threads()) # Reset if changed\n",
    "        print(f\"Average INT8 inference time: {int8_avg_time:.3f} ms per batch\")\n",
    "\n",
    "        # --- Calculate and print speedup ---\n",
    "        if int8_avg_time > 0:\n",
    "            speedup_factor = fp32_avg_time / int8_avg_time\n",
    "            print(f\"\\nInference speedup factor (INT8 vs FP32 on CPU): {speedup_factor:.2f}x\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate speedup factor (INT8 average time was zero or invalid).\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nError: Could not get a batch from calibration_loader. Was it exhausted?\")\n",
    "        # Try re-initializing the iterator if needed\n",
    "        # (Add code here to re-create calibration_loader if necessary)\n",
    "\n",
    "    except RuntimeError as e_runtime:\n",
    "        # This is the error we were trying to fix. If it still occurs,\n",
    "        # there might be deeper issues (backend install, unsupported ops).\n",
    "        print(f\"\\nRuntimeError during inference timing: {e_runtime}\")\n",
    "        print(\"This might indicate backend incompatibility or missing kernels even with stubs.\")\n",
    "        print(f\"Verify backend '{current_backend}' support in your PyTorch installation.\")\n",
    "        print(\"Check if all operations in the model are supported for quantization with this backend.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during inference timing: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping inference speed comparison because conversion step failed or was skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
