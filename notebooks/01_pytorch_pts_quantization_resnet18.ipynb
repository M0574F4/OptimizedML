{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version: 2.6.0+cu124\n",
      "Torchvision Version: 0.21.0+cu124\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports (Add quantization models)\n",
    "# ========================================\n",
    "import torch\n",
    "import torchvision\n",
    "# --- MODIFIED IMPORT ---\n",
    "# Import both standard and quantization models if needed for comparison,\n",
    "# or just quantization models if only quantizing.\n",
    "from torchvision.models import ResNet18_Weights # For FP32 weights/transforms\n",
    "import torchvision.models.quantization as models_quant # For quantization-ready model structure\n",
    "# --- END MODIFICATION ---\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\") # Will use CPU for quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing transforms for model loaded.\n",
      "Quantization-ready FP32 ResNet18 model loaded and moved to CPU.\n",
      "Original FP32 ResNet18 model loaded.\n",
      "Preprocessing transforms for model loaded.\n",
      "Model moved to CPU for quantization steps.\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load Model (Modified to load quantization-ready structure)\n",
    "# ==================================================================\n",
    "def load_quantization_ready_model():\n",
    "    \"\"\"Loads the ResNet18 model structure prepared for quantization,\n",
    "       initialized with standard FP32 weights.\"\"\"\n",
    "    # Get weights and transforms from the standard FP32 model\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    preprocess = weights.transforms()\n",
    "    print(\"Preprocessing transforms for model loaded.\")\n",
    "\n",
    "    # --- MODIFIED MODEL LOADING ---\n",
    "    # Load the *quantization-ready* model structure from torchvision.models.quantization\n",
    "    # pretrained=True loads the standard FP32 weights into this structure.\n",
    "    # quantize=False ensures it's not quantized yet, just ready for prepare/calibration.\n",
    "    model = models_quant.resnet18(weights=weights, quantize=False)\n",
    "    # --- END MODIFICATION ---\n",
    "\n",
    "    model.eval() # Set model to evaluation mode\n",
    "    model.cpu()  # Ensure model is on CPU for quantization steps\n",
    "    print(\"Quantization-ready FP32 ResNet18 model loaded and moved to CPU.\")\n",
    "    return model, preprocess\n",
    "\n",
    "# Load the model prepared for quantization\n",
    "# This model already includes QuantStub/DeQuantStub internally\n",
    "fp32_model_quant_ready, preprocess = load_quantization_ready_model()\n",
    "\n",
    "# Optional: Keep a true original FP32 model for comparison if needed\n",
    "# fp32_model_original = torchvision.models.resnet18(weights=ResNet18_Weights.DEFAULT).cpu().eval()\n",
    "# print(\"Original standard FP32 ResNet18 loaded for comparison.\")\n",
    "\n",
    "# Use the quantization-ready model for the rest of the process\n",
    "fp32_model_to_quantize = fp32_model_quant_ready # Renaming for clarity in subsequent steps\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the pre-trained FP32 ResNet18 model.\"\"\"\n",
    "    # Use the recommended weights parameter for pretrained models\n",
    "    weights = ResNet18_Weights.DEFAULT # Loads the best available weights (ImageNet V1)\n",
    "    model = torchvision.models.resnet18(weights=weights)\n",
    "    model.eval() # Set model to evaluation mode! Important for quantization.\n",
    "    # Note: For static quantization, we usually target CPU execution.\n",
    "    # We'll ensure the model is on the CPU before quantization steps.\n",
    "    # model.to(device) # We can load it to device later if needed for FP32 comparison\n",
    "    print(\"Original FP32 ResNet18 model loaded.\")\n",
    "    # Get the transformation pipeline associated with the weights\n",
    "    # This includes resizing, center cropping, normalization, etc.\n",
    "    preprocess = weights.transforms()\n",
    "    print(\"Preprocessing transforms for model loaded.\")\n",
    "    return model, preprocess\n",
    "\n",
    "# Load the floating-point model\n",
    "fp32_model, preprocess = load_model()\n",
    "\n",
    "# Ensure model is on CPU for quantization compatibility checks later\n",
    "fp32_model.cpu()\n",
    "print(f\"Model moved to CPU for quantization steps.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_size(model, label=\"\"):\n",
    "    \"\"\"Saves the model's state_dict temporarily and prints its size.\"\"\"\n",
    "    # Create a temporary file path\n",
    "    temp_file_path = \"temp_model_state.pth\"\n",
    "    # Save the state dictionary\n",
    "    torch.save(model.state_dict(), temp_file_path)\n",
    "    # Get the file size in bytes and convert to megabytes\n",
    "    size_bytes = os.path.getsize(temp_file_path)\n",
    "    size_mb = size_bytes / (1024 * 1024)\n",
    "    # Print the formatted size\n",
    "    print(f\"{label} Model size: {size_mb:.2f} MB\")\n",
    "    # Remove the temporary file\n",
    "    os.remove(temp_file_path)\n",
    "    # Return the size in MB\n",
    "    return size_mb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Configuring Quantization ---\n",
      "Quantization backend set to: fbgemm\n",
      "Quantization configuration applied to the model.\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Configure Quantization (Mostly unchanged, but applied to the new model type)\n",
    "# ==================================================================================\n",
    "print(\"\\n--- Configuring Quantization ---\")\n",
    "\n",
    "# Make a copy of the quantization-ready model\n",
    "# This copy already has the right structure (stubs etc.)\n",
    "quantized_model = copy.deepcopy(fp32_model_to_quantize)\n",
    "quantized_model.eval()\n",
    "\n",
    "# --- Backend setup remains the same ---\n",
    "q_backend = \"none\"\n",
    "if 'fbgemm' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"fbgemm\"\n",
    "elif 'qnnpack' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"qnnpack\"\n",
    "else:\n",
    "    print(\"Warning: Neither 'fbgemm' nor 'qnnpack' supported.\")\n",
    "\n",
    "qconfig = None\n",
    "if q_backend != \"none\":\n",
    "    try:\n",
    "        qconfig = torch.quantization.get_default_qconfig(q_backend)\n",
    "        torch.backends.quantized.engine = q_backend\n",
    "        print(f\"Quantization backend set to: {q_backend}\")\n",
    "\n",
    "        # Apply the qconfig to the model instance\n",
    "        # This might be redundant for torchvision quantization models, but safe.\n",
    "        quantized_model.qconfig = qconfig\n",
    "        print(\"Quantization configuration applied to the model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up quantization backend {q_backend}: {e}\")\n",
    "        qconfig = None\n",
    "else:\n",
    "    print(\"Skipping quantization due to lack of supported backend.\")\n",
    "\n",
    "# --- No explicit fusion needed here ---\n",
    "# Torchvision's quantization models typically handle necessary fusions\n",
    "# implicitly or are structured correctly already. Avoid manual fuse_modules.\n",
    "# print(\"Skipping manual fusion; using torchvision quantization model structure.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Checking Initial Model Size ---\n",
      "FP32 Model size: 44.67 MB\n"
     ]
    }
   ],
   "source": [
    "# Check the size of the original FP32 model\n",
    "print(\"\\n--- Checking Initial Model Size ---\")\n",
    "fp32_model_size = print_model_size(fp32_model, \"FP32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Configuring Quantization ---\n",
      "Quantization backend set to: fbgemm\n",
      "Quantization configuration applied to the model.\n"
     ]
    }
   ],
   "source": [
    "# --- Configure Quantization ---\n",
    "print(\"\\n--- Configuring Quantization ---\")\n",
    "\n",
    "# Make a copy of the model for quantization to preserve the original fp32 model\n",
    "quantized_model = copy.deepcopy(fp32_model)\n",
    "quantized_model.eval() # Ensure evaluation mode\n",
    "\n",
    "# Specify quantization configuration\n",
    "# 'fbgemm' is generally recommended for x86 server/desktop CPUs\n",
    "# 'qnnpack' is generally recommended for ARM mobile CPUs\n",
    "# We check for availability and set the backend engine\n",
    "q_backend = \"none\"\n",
    "if 'fbgemm' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"fbgemm\"\n",
    "elif 'qnnpack' in torch.backends.quantized.supported_engines:\n",
    "    q_backend = \"qnnpack\"\n",
    "else:\n",
    "    print(\"Warning: Neither 'fbgemm' nor 'qnnpack' supported. Static quantization might not work well.\")\n",
    "\n",
    "qconfig = None\n",
    "if q_backend != \"none\":\n",
    "    try:\n",
    "        # Get the default static quantization configuration for the chosen backend\n",
    "        qconfig = torch.quantization.get_default_qconfig(q_backend)\n",
    "        torch.backends.quantized.engine = q_backend\n",
    "        print(f\"Quantization backend set to: {q_backend}\")\n",
    "\n",
    "        # Apply the qconfig to the model instance (this is needed for prepare)\n",
    "        quantized_model.qconfig = qconfig\n",
    "        print(\"Quantization configuration applied to the model.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error setting up quantization backend {q_backend}: {e}\")\n",
    "        qconfig = None # Ensure qconfig is None if setup fails\n",
    "\n",
    "else:\n",
    "    print(\"Skipping quantization due to lack of supported backend.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Calibration Data ---\n",
      "Using 500 images from CIFAR10 for calibration.\n",
      "Calibration DataLoader created with batch size 32.\n",
      "Sample batch tensor shape: torch.Size([32, 3, 224, 224]), dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# --- Prepare Calibration Data ---\n",
    "print(\"\\n--- Preparing Calibration Data ---\")\n",
    "\n",
    "# Use CIFAR10 dataset for calibration images\n",
    "# The 'preprocess' pipeline obtained earlier from ResNet18_Weights.DEFAULT\n",
    "# includes the necessary Resize, CenterCrop, ToTensor, and Normalize steps.\n",
    "calibration_transform = preprocess\n",
    "\n",
    "# Create a directory to store the data if it doesn't exist\n",
    "data_dir = './data'\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "    print(f\"Created directory: {data_dir}\")\n",
    "\n",
    "calibration_loader = None\n",
    "try:\n",
    "    # Download/Load CIFAR10 training set\n",
    "    calibration_dataset_full = datasets.CIFAR10(\n",
    "        root=data_dir,\n",
    "        train=True,       # Use training images\n",
    "        download=True,    # Download if not present\n",
    "        transform=calibration_transform # Apply ResNet18 preprocessing\n",
    "    )\n",
    "\n",
    "    # Create a subset for faster calibration (e.g., first 500 images)\n",
    "    num_calibration_images = 500\n",
    "    calibration_subset_indices = list(range(num_calibration_images))\n",
    "    calibration_dataset = torch.utils.data.Subset(calibration_dataset_full, calibration_subset_indices)\n",
    "\n",
    "    # Create a DataLoader\n",
    "    # Adjust batch_size and num_workers based on your system capabilities\n",
    "    calibration_loader = torch.utils.data.DataLoader(\n",
    "        calibration_dataset,\n",
    "        batch_size=32,\n",
    "        shuffle=False, # No need to shuffle for calibration\n",
    "        num_workers=2  # Use background workers for data loading if possible\n",
    "    )\n",
    "\n",
    "    print(f\"Using {len(calibration_dataset)} images from CIFAR10 for calibration.\")\n",
    "    print(f\"Calibration DataLoader created with batch size {calibration_loader.batch_size}.\")\n",
    "\n",
    "    # Optional: Verify one batch shape and type\n",
    "    images, _ = next(iter(calibration_loader))\n",
    "    print(f\"Sample batch tensor shape: {images.shape}, dtype: {images.dtype}\") # Should be [batch_size, 3, height, width] e.g. [32, 3, 224, 224]\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError loading or processing calibration data: {e}\")\n",
    "    print(\"Please ensure network connectivity if downloading for the first time, or check dataset integrity.\")\n",
    "    # Ensure loader is None if data loading fails\n",
    "    calibration_loader = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Preparing Model for Static Quantization ---\n",
      "Model prepared for static quantization (observers inserted).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/Mostafa/OptimizedML/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Prepare Model for Static Quantization (Logic Unchanged)\n",
    "# ==============================================================\n",
    "print(\"\\n--- Preparing Model for Static Quantization ---\")\n",
    "\n",
    "prepared_model_ready = False\n",
    "# Ensure qconfig was set and model has it (redundant check is fine)\n",
    "if qconfig and calibration_loader and hasattr(quantized_model, 'qconfig'):\n",
    "    # Ensure model is on CPU before preparing\n",
    "    quantized_model.cpu()\n",
    "    quantized_model.eval() # Ensure eval mode\n",
    "\n",
    "    # --- Prepare call remains the same ---\n",
    "    # It will now correctly insert observers into the quantization-ready structure\n",
    "    torch.quantization.prepare(quantized_model, inplace=True)\n",
    "    print(\"Model prepared for static quantization (observers inserted).\")\n",
    "    prepared_model_ready = True\n",
    "else:\n",
    "    print(\"Skipping model preparation: Check quantization config, calibration data, and backend support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calibrating the Model ---\n",
      "Running calibration data through the prepared model...\n",
      "  Calibration batch 16/16 processed.\n",
      "Calibration finished. Activation statistics collected by observers.\n"
     ]
    }
   ],
   "source": [
    "# --- Calibrate the Model ---\n",
    "print(\"\\n--- Calibrating the Model ---\")\n",
    "\n",
    "calibration_done = False\n",
    "if prepared_model_ready:\n",
    "    print(\"Running calibration data through the prepared model...\")\n",
    "    # Ensure model is on CPU and in eval mode\n",
    "    quantized_model.cpu()\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Run data through the prepared model to allow observers to collect statistics\n",
    "    with torch.no_grad(): # Important: Disable gradient calculation\n",
    "        for i, (images, _) in enumerate(calibration_loader):\n",
    "            # Move data to CPU (model is on CPU)\n",
    "            images_cpu = images.to('cpu')\n",
    "            # Pass data through the model\n",
    "            quantized_model(images_cpu)\n",
    "            # Print progress indicator\n",
    "            print(f\"  Calibration batch {i+1}/{len(calibration_loader)} processed.\", end='\\r')\n",
    "\n",
    "    print(\"\\nCalibration finished. Activation statistics collected by observers.\")\n",
    "    calibration_done = True\n",
    "else:\n",
    "    print(\"Skipping calibration step because model was not prepared.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting the Model to Quantized INT8 ---\n",
      "Model successfully converted to INT8 quantized format.\n"
     ]
    }
   ],
   "source": [
    "# --- Convert Model to Quantized INT8 ---\n",
    "print(\"\\n--- Converting the Model to Quantized INT8 ---\")\n",
    "\n",
    "int8_model = None # Initialize variable\n",
    "conversion_done = False\n",
    "if calibration_done:\n",
    "    # Ensure model is on CPU before conversion\n",
    "    quantized_model.cpu()\n",
    "    # Convert the calibrated model (observers -> quantized modules)\n",
    "    # inplace=True modifies the quantized_model directly\n",
    "    # Note: quantized_model was the deepcopy we made earlier\n",
    "    try:\n",
    "        torch.quantization.convert(quantized_model, inplace=True)\n",
    "        int8_model = quantized_model # Assign the converted model for clarity\n",
    "        print(\"Model successfully converted to INT8 quantized format.\")\n",
    "        conversion_done = True\n",
    "\n",
    "        # Optional: Print the structure of the quantized model\n",
    "        # print(\"\\nStructure of the INT8 Model:\")\n",
    "        # print(int8_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model conversion: {e}\")\n",
    "        conversion_done = False\n",
    "else:\n",
    "    print(\"Skipping conversion because calibration was not completed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Comparing Model Sizes ---\n",
      "Original FP32 Model:\n",
      "FP32 Model size: 44.67 MB\n",
      "\n",
      "Quantized INT8 Model:\n",
      "INT8 Model size: 11.38 MB\n",
      "\n",
      "Size reduction factor: 3.93x\n",
      "Model size reduced from 44.67 MB to 11.38 MB.\n"
     ]
    }
   ],
   "source": [
    "# --- Compare Model Sizes ---\n",
    "print(\"\\n--- Comparing Model Sizes ---\")\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    print(\"Original FP32 Model:\")\n",
    "    # We stored fp32_model_size earlier, but let's recalculate for direct comparison here\n",
    "    fp32_model_size = print_model_size(fp32_model, \"FP32\")\n",
    "\n",
    "    print(\"\\nQuantized INT8 Model:\")\n",
    "    int8_model_size = print_model_size(int8_model, \"INT8\")\n",
    "\n",
    "    # Calculate and print size reduction\n",
    "    if int8_model_size > 0: # Avoid division by zero\n",
    "      size_reduction = fp32_model_size / int8_model_size\n",
    "      print(f\"\\nSize reduction factor: {size_reduction:.2f}x\")\n",
    "      print(f\"Model size reduced from {fp32_model_size:.2f} MB to {int8_model_size:.2f} MB.\")\n",
    "    else:\n",
    "      print(\"\\nCould not calculate size reduction (INT8 model size is zero or invalid).\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping size comparison because conversion step failed or was skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Verifying Runtime Environment ---\n",
      "Python Executable: /project_ghent/Mostafa/OptimizedML/.venv/bin/python\n",
      "PyTorch Version: 2.6.0+cu124\n",
      "Torchvision Version: 0.21.0+cu124\n",
      "CUDA Available: True\n",
      "PyTorch CUDA Version: 12.4\n",
      "Device Name: NVIDIA GeForce RTX 4090\n",
      "Quantization Backend Before Setting: fbgemm\n",
      "\n",
      "--- Comparing Inference Speed (CPU) ---\n",
      "Quantization backend explicitly set to: qnnpack\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'conversion_done' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     34\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mWarning: Could not set quantization backend \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_backend\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCurrently used backend: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch.backends.quantized.engine\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconversion_done\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m int8_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Ensure both models are on CPU and in eval mode for a fair comparison\u001b[39;00m\n\u001b[32m     40\u001b[39m     fp32_model.cpu().eval()\n\u001b[32m     41\u001b[39m     int8_model.cpu().eval() \u001b[38;5;66;03m# Already on CPU and eval from conversion step\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'conversion_done' is not defined"
     ]
    }
   ],
   "source": [
    "# Cell 10.5: Verify PyTorch Runtime Environment\n",
    "# ============================================\n",
    "# Add this cell right before the speed comparison cell (Cell 11)\n",
    "import torch\n",
    "import sys\n",
    "\n",
    "print(\"\\n--- Verifying Runtime Environment ---\")\n",
    "print(f\"Python Executable: {sys.executable}\")\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "print(f\"Torchvision Version: {torchvision.__version__}\") # Assuming torchvision is imported\n",
    "print(f\"CUDA Available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"PyTorch CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"Device Name: {torch.cuda.get_device_name(0)}\")\n",
    "print(f\"Quantization Backend Before Setting: {torch.backends.quantized.engine}\")\n",
    "\n",
    "\n",
    "# Cell 11: Compare Inference Speed (CPU) - UPDATED to use QNNPACK\n",
    "# ==============================================================\n",
    "# Note: Speedup depends heavily on CPU architecture, PyTorch version,\n",
    "# backend support (fbgemm/qnnpack), and whether the CPU has specific\n",
    "# INT8 acceleration instructions (like AVX2/AVX512 VNNI).\n",
    "\n",
    "print(\"\\n--- Comparing Inference Speed (CPU) ---\")\n",
    "\n",
    "# --- Explicitly set quantization backend ---\n",
    "# Switching back to 'qnnpack' as 'fbgemm' consistently failed.\n",
    "try:\n",
    "    # current_backend = 'fbgemm'\n",
    "    current_backend = 'qnnpack' # << TRYING QNNPACK AGAIN\n",
    "    torch.backends.quantized.engine = current_backend\n",
    "    print(f\"Quantization backend explicitly set to: {torch.backends.quantized.engine}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not set quantization backend '{current_backend}'. Error: {e}\")\n",
    "    print(f\"Currently used backend: {torch.backends.quantized.engine}\")\n",
    "\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    # Ensure both models are on CPU and in eval mode for a fair comparison\n",
    "    fp32_model.cpu().eval()\n",
    "    int8_model.cpu().eval() # Already on CPU and eval from conversion step\n",
    "\n",
    "    # Create a sample input tensor (using one batch from the calibration loader)\n",
    "    # Make sure it's on the CPU\n",
    "    try:\n",
    "        # Get a batch from the loader (ensure it's still available)\n",
    "        # Re-create iterator in case it was exhausted\n",
    "        calib_iter = iter(calibration_loader)\n",
    "        sample_input, _ = next(calib_iter)\n",
    "        sample_input_cpu = sample_input.to('cpu')\n",
    "        print(f\"Using sample input batch of shape: {sample_input_cpu.shape} on CPU for timing.\")\n",
    "\n",
    "        # Helper function to time inference runs accurately\n",
    "        def time_model_inference(model, input_tensor, num_runs=50, warm_up=10):\n",
    "            \"\"\"Times model inference, returning average time in milliseconds.\"\"\"\n",
    "            model.eval() # Ensure evaluation mode\n",
    "            model.to('cpu') # Ensure model is on CPU\n",
    "            input_tensor = input_tensor.to('cpu') # Ensure input is on CPU\n",
    "\n",
    "            with torch.no_grad(): # Disable gradient calculations for inference\n",
    "                # Warm-up runs\n",
    "                print(f\"  Performing {warm_up} warm-up runs...\")\n",
    "                for _ in range(warm_up):\n",
    "                    _ = model(input_tensor)\n",
    "\n",
    "                # Timed runs\n",
    "                print(f\"  Performing {num_runs} timed runs...\")\n",
    "                start_time = time.time()\n",
    "                for _ in range(num_runs):\n",
    "                    _ = model(input_tensor)\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            avg_time_ms = (total_time / num_runs) * 1000 # Average time in milliseconds\n",
    "            return avg_time_ms\n",
    "\n",
    "        # --- Time FP32 model inference ---\n",
    "        print(\"\\nTiming FP32 model inference...\")\n",
    "        fp32_avg_time = time_model_inference(fp32_model, sample_input_cpu)\n",
    "        print(f\"Average FP32 inference time: {fp32_avg_time:.3f} ms per batch\")\n",
    "\n",
    "        # --- Time INT8 model inference ---\n",
    "        print(\"\\nTiming INT8 model inference...\")\n",
    "        # Ensure PyTorch threading is set for optimal performance (often helps INT8)\n",
    "        # torch.set_num_threads(1) # Uncomment if you want to test single-thread performance\n",
    "        int8_avg_time = time_model_inference(int8_model, sample_input_cpu)\n",
    "        print(f\"Average INT8 inference time: {int8_avg_time:.3f} ms per batch\")\n",
    "        # torch.set_num_threads(torch.get_num_threads()) # Reset if you changed it\n",
    "\n",
    "        # --- Calculate and print speedup ---\n",
    "        if int8_avg_time > 0: # Avoid division by zero\n",
    "            speedup_factor = fp32_avg_time / int8_avg_time\n",
    "            print(f\"\\nInference speedup factor (INT8 vs FP32 on CPU): {speedup_factor:.2f}x\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate speedup factor (INT8 average time was zero or invalid).\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nError: Could not get a batch from calibration_loader. Was it exhausted?\")\n",
    "        # Try re-initializing the iterator if needed\n",
    "        try:\n",
    "            print(\"Attempting to re-initialize calibration loader iterator...\")\n",
    "            # Ensure calibration_dataset is still defined from earlier cells\n",
    "            calibration_loader = torch.utils.data.DataLoader(calibration_dataset, batch_size=32, shuffle=False)\n",
    "            calib_iter = iter(calibration_loader)\n",
    "            sample_input, _ = next(calib_iter)\n",
    "            sample_input_cpu = sample_input.to('cpu')\n",
    "            print(\"Successfully re-initialized iterator and got sample input. Please re-run the timing cell.\")\n",
    "        except NameError:\n",
    "             print(\"Failed to re-initialize loader: 'calibration_dataset' not found. Please re-run previous cells.\")\n",
    "        except Exception as e_retry:\n",
    "             print(f\"Failed to re-initialize or get data: {e_retry}\")\n",
    "\n",
    "    except RuntimeError as e_runtime:\n",
    "        print(f\"\\nRuntimeError during inference timing: {e_runtime}\")\n",
    "        print(\"This often indicates a backend incompatibility or missing kernels.\")\n",
    "        print(f\"Current backend used for INT8 timing: {torch.backends.quantized.engine}\")\n",
    "        print(\"Even after reinstalling PyTorch and trying both backends, the error persists.\")\n",
    "        print(\"Consider checking CPU instruction set support (e.g., AVX2, VNNI using 'lscpu' or similar tools).\")\n",
    "        print(\"Alternatively, we could focus on accuracy evaluation instead of speed benchmarking.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during inference timing: {e}\")\n",
    "        print(\"Ensure calibration_loader is accessible and models are valid.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping inference speed comparison because conversion step failed or was skipped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Converting the Model to Quantized INT8 ---\n",
      "Attempting conversion...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/project_ghent/Mostafa/OptimizedML/.venv/lib/python3.11/site-packages/torch/ao/quantization/observer.py:1318: UserWarning: must run observer before calling calculate_qparams.                                    Returning default scale and zero point \n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model successfully converted to INT8 quantized format.\n",
      "\n",
      "--- Comparing Model Sizes ---\n",
      "Original FP32 Model:\n",
      "FP32 Model size: 44.67 MB\n",
      "\n",
      "Quantized INT8 Model:\n",
      "INT8 Model size: 11.41 MB\n",
      "\n",
      "Size reduction factor: 3.92x\n",
      "Model size reduced from 44.67 MB to 11.41 MB.\n",
      "\n",
      "--- Comparing Inference Speed (CPU) ---\n",
      "Quantization backend set to: fbgemm\n",
      "Using sample input batch of shape: torch.Size([32, 3, 224, 224]) on CPU for timing.\n",
      "\n",
      "Timing FP32 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "Average FP32 inference time: 8716.243 ms per batch\n",
      "\n",
      "Timing INT8 model inference...\n",
      "  Performing 10 warm-up runs...\n",
      "  Performing 50 timed runs...\n",
      "Average INT8 inference time: 18950.671 ms per batch\n",
      "\n",
      "Inference speedup factor (INT8 vs FP32 on CPU): 0.46x\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Convert Model to Quantized INT8\n",
    "# =======================================\n",
    "import torch\n",
    "import torch.quantization\n",
    "import time # Needed for speed comparison later\n",
    "import os   # Needed for print_model_size if not already imported\n",
    "\n",
    "# --- Assume previous cells defined: ---\n",
    "# fp32_model: The original loaded FP32 model\n",
    "# quantized_model: The deepcopy of fp32_model prepared for quantization\n",
    "# calibration_loader: The DataLoader used for calibration\n",
    "# calibration_done: A boolean flag indicating if calibration succeeded (set this based on previous cell output)\n",
    "# print_model_size: The helper function to print model size\n",
    "\n",
    "# --- Set calibration_done based on your previous output ---\n",
    "# If the calibration cell ran without errors, set this to True\n",
    "calibration_done = True # <<< SET THIS MANUALLY based on previous cell output\n",
    "\n",
    "print(\"\\n--- Converting the Model to Quantized INT8 ---\")\n",
    "\n",
    "int8_model = None # Initialize variable to hold the final INT8 model\n",
    "conversion_done = False # Flag to track if conversion was successful\n",
    "\n",
    "if calibration_done:\n",
    "    # Ensure the model prepared for quantization is on the CPU before conversion\n",
    "    quantized_model.cpu()\n",
    "    # Make sure it's in eval mode\n",
    "    quantized_model.eval()\n",
    "\n",
    "    # Convert the calibrated model (observers -> quantized modules)\n",
    "    # inplace=True modifies the quantized_model directly\n",
    "    # Note: quantized_model was the deepcopy we made earlier\n",
    "    try:\n",
    "        print(\"Attempting conversion...\")\n",
    "        # The core conversion step\n",
    "        torch.quantization.convert(quantized_model, inplace=True)\n",
    "\n",
    "        # Assign the converted model to int8_model for clarity\n",
    "        int8_model = quantized_model\n",
    "        print(\"Model successfully converted to INT8 quantized format.\")\n",
    "        conversion_done = True\n",
    "\n",
    "        # Optional: Print the structure of the quantized model to see the changes\n",
    "        # print(\"\\nStructure of the INT8 Model:\")\n",
    "        # print(int8_model)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during model conversion: {e}\")\n",
    "        conversion_done = False\n",
    "else:\n",
    "    print(\"Skipping conversion because calibration was not marked as completed successfully.\")\n",
    "\n",
    "# Cell 10: Compare Model Sizes\n",
    "# ============================\n",
    "\n",
    "# --- Ensure print_model_size function is defined ---\n",
    "# (Include if not defined in a previous cell)\n",
    "def print_model_size(model, label=\"\"):\n",
    "    \"\"\"Helper function to save model, check file size, and print.\"\"\"\n",
    "    # Save the model temporarily to calculate its size\n",
    "    temp_filename = f\"{label}_temp_model_state.pt\"\n",
    "    torch.save(model.state_dict(), temp_filename)\n",
    "    size_mb = os.path.getsize(temp_filename) / (1024 * 1024)\n",
    "    os.remove(temp_filename) # Clean up the temporary file\n",
    "    print(f\"{label} Model size: {size_mb:.2f} MB\")\n",
    "    return size_mb\n",
    "\n",
    "print(\"\\n--- Comparing Model Sizes ---\")\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    print(\"Original FP32 Model:\")\n",
    "    # Recalculate or use stored value if available\n",
    "    fp32_model_size = print_model_size(fp32_model, \"FP32\")\n",
    "\n",
    "    print(\"\\nQuantized INT8 Model:\")\n",
    "    int8_model_size = print_model_size(int8_model, \"INT8\")\n",
    "\n",
    "    # Calculate and print size reduction\n",
    "    if fp32_model_size > 0 and int8_model_size > 0: # Avoid division by zero\n",
    "        size_reduction = fp32_model_size / int8_model_size\n",
    "        print(f\"\\nSize reduction factor: {size_reduction:.2f}x\")\n",
    "        print(f\"Model size reduced from {fp32_model_size:.2f} MB to {int8_model_size:.2f} MB.\")\n",
    "    else:\n",
    "        print(\"\\nCould not calculate size reduction (one or both model sizes are zero or invalid).\")\n",
    "else:\n",
    "    print(\"Skipping size comparison because conversion step failed or was skipped.\")\n",
    "\n",
    "\n",
    "# Cell 11: Compare Inference Speed (CPU) - UPDATED\n",
    "# ================================================\n",
    "# Note: Speedup depends heavily on CPU architecture, PyTorch version,\n",
    "# backend support (fbgemm/qnnpack), and whether the CPU has specific\n",
    "# INT8 acceleration instructions (like AVX2/AVX512 VNNI).\n",
    "\n",
    "print(\"\\n--- Comparing Inference Speed (CPU) ---\")\n",
    "\n",
    "# --- Explicitly set quantization backend ---\n",
    "# Setting back to 'fbgemm' as recommended for x86 CPUs.\n",
    "try:\n",
    "    current_backend = 'fbgemm' # << SWITCHING BACK TO FBGEMM\n",
    "    # current_backend = 'qnnpack'\n",
    "    torch.backends.quantized.engine = current_backend\n",
    "    print(f\"Quantization backend set to: {torch.backends.quantized.engine}\")\n",
    "except Exception as e:\n",
    "    print(f\"Warning: Could not set quantization backend '{current_backend}'. Error: {e}\")\n",
    "    print(f\"Currently used backend: {torch.backends.quantized.engine}\")\n",
    "\n",
    "\n",
    "if conversion_done and int8_model is not None:\n",
    "    # Ensure both models are on CPU and in eval mode for a fair comparison\n",
    "    fp32_model.cpu().eval()\n",
    "    int8_model.cpu().eval() # Already on CPU and eval from conversion step\n",
    "\n",
    "    # Create a sample input tensor (using one batch from the calibration loader)\n",
    "    # Make sure it's on the CPU\n",
    "    try:\n",
    "        # Get a batch from the loader (ensure it's still available)\n",
    "        # Re-create iterator in case it was exhausted\n",
    "        calib_iter = iter(calibration_loader)\n",
    "        sample_input, _ = next(calib_iter)\n",
    "        sample_input_cpu = sample_input.to('cpu')\n",
    "        print(f\"Using sample input batch of shape: {sample_input_cpu.shape} on CPU for timing.\")\n",
    "\n",
    "        # Helper function to time inference runs accurately\n",
    "        def time_model_inference(model, input_tensor, num_runs=50, warm_up=10):\n",
    "            \"\"\"Times model inference, returning average time in milliseconds.\"\"\"\n",
    "            model.eval() # Ensure evaluation mode\n",
    "            model.to('cpu') # Ensure model is on CPU\n",
    "            input_tensor = input_tensor.to('cpu') # Ensure input is on CPU\n",
    "\n",
    "            with torch.no_grad(): # Disable gradient calculations for inference\n",
    "                # Warm-up runs\n",
    "                print(f\"  Performing {warm_up} warm-up runs...\")\n",
    "                for _ in range(warm_up):\n",
    "                    _ = model(input_tensor)\n",
    "\n",
    "                # Timed runs\n",
    "                print(f\"  Performing {num_runs} timed runs...\")\n",
    "                start_time = time.time()\n",
    "                for _ in range(num_runs):\n",
    "                    _ = model(input_tensor)\n",
    "                end_time = time.time()\n",
    "\n",
    "            total_time = end_time - start_time\n",
    "            avg_time_ms = (total_time / num_runs) * 1000 # Average time in milliseconds\n",
    "            return avg_time_ms\n",
    "\n",
    "        # --- Time FP32 model inference ---\n",
    "        print(\"\\nTiming FP32 model inference...\")\n",
    "        fp32_avg_time = time_model_inference(fp32_model, sample_input_cpu)\n",
    "        print(f\"Average FP32 inference time: {fp32_avg_time:.3f} ms per batch\")\n",
    "\n",
    "        # --- Time INT8 model inference ---\n",
    "        print(\"\\nTiming INT8 model inference...\")\n",
    "        # Ensure PyTorch threading is set for optimal performance (often helps INT8)\n",
    "        # torch.set_num_threads(1) # Uncomment if you want to test single-thread performance\n",
    "        int8_avg_time = time_model_inference(int8_model, sample_input_cpu)\n",
    "        print(f\"Average INT8 inference time: {int8_avg_time:.3f} ms per batch\")\n",
    "        # torch.set_num_threads(torch.get_num_threads()) # Reset if you changed it\n",
    "\n",
    "        # --- Calculate and print speedup ---\n",
    "        if int8_avg_time > 0: # Avoid division by zero\n",
    "            speedup_factor = fp32_avg_time / int8_avg_time\n",
    "            print(f\"\\nInference speedup factor (INT8 vs FP32 on CPU): {speedup_factor:.2f}x\")\n",
    "        else:\n",
    "            print(\"\\nCould not calculate speedup factor (INT8 average time was zero or invalid).\")\n",
    "\n",
    "    except StopIteration:\n",
    "        print(\"\\nError: Could not get a batch from calibration_loader. Was it exhausted?\")\n",
    "        # Try re-initializing the iterator if needed\n",
    "        try:\n",
    "            print(\"Attempting to re-initialize calibration loader iterator...\")\n",
    "            calibration_loader = torch.utils.data.DataLoader(calibration_dataset, batch_size=32, shuffle=False) # Recreate if necessary\n",
    "            calib_iter = iter(calibration_loader)\n",
    "            sample_input, _ = next(calib_iter)\n",
    "            sample_input_cpu = sample_input.to('cpu')\n",
    "            print(\"Successfully re-initialized iterator and got sample input. Please re-run the timing cell.\")\n",
    "        except Exception as e_retry:\n",
    "             print(f\"Failed to re-initialize or get data: {e_retry}\")\n",
    "\n",
    "    except RuntimeError as e_runtime:\n",
    "        print(f\"\\nRuntimeError during inference timing: {e_runtime}\")\n",
    "        print(\"This often indicates a backend incompatibility or missing kernels.\")\n",
    "        print(f\"Current backend: {torch.backends.quantized.engine}\")\n",
    "        if torch.backends.quantized.engine == 'fbgemm':\n",
    "            print(\"Suggestion: Ensure PyTorch was installed correctly with support for fbgemm quantized kernels for your CPU.\")\n",
    "            print(\"If issues persist, try the 'qnnpack' backend again, although it also failed previously.\")\n",
    "        else: # qnnpack\n",
    "             print(\"Suggestion: Ensure PyTorch was installed correctly with support for qnnpack quantized kernels for your CPU/ARM architecture.\")\n",
    "             print(\"If issues persist, try the 'fbgemm' backend again, although it also failed previously.\")\n",
    "        print(\"Consider checking PyTorch installation guides or potentially reinstalling.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn unexpected error occurred during inference timing: {e}\")\n",
    "        print(\"Ensure calibration_loader is accessible and models are valid.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping inference speed comparison because conversion step failed or was skipped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
